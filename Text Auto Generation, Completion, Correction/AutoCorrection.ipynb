{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda0930c-6e59-411a-a84b-325730c6edbf",
   "metadata": {},
   "source": [
    "# CORRECTEUR D'ORTHOGRAPHE 2\n",
    "\n",
    "\n",
    "|<h2>Chargement des données textuelles à partir du fichier big.txt</h2> |\n",
    "| :-------------------------------------------------------------------------: |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efb95cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "from math import log, exp\n",
    "\n",
    "\n",
    "#Charger les données\n",
    "def load_data(nom_fichier: str) -> str:\n",
    "  \n",
    "    donnees = []\n",
    "    with open(nom_fichier, \"r\", encoding='utf-8') as f:\n",
    "        donnees = f.read()\n",
    "\n",
    "    return donnees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c5600-9e25-4412-9123-4112f8668f7b",
   "metadata": {},
   "source": [
    "|<h2>Tokenisation des données textuelles</h2> |\n",
    "| :------------------: |\n",
    "| La fonction process_data effectue une tokenisation des données. Elle commence par diviser les données en phrases, puis elle tokenise chaque phrase individuellement à l'aide de la fonction tokenizer_phrase. Le résultat est un tuple contenant la liste des phrases originales et une liste de listes, où chaque sous-liste représente les tokens d'une phrase spécifique.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e91b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(donnees: str) -> tuple[list[str], list[list[str]]]:\n",
    "    \"\"\"\n",
    "    Tokenise les données en phrases et en mots.\n",
    "\n",
    "    Paramètres :\n",
    "        donnees (str) : Les données d'entrée sous forme de chaîne de caractères.\n",
    "\n",
    "    Retourne :\n",
    "        tuple[list[str], list[list[str]]] : Un tuple contenant la liste des phrases et la liste des phrases tokenisées.\n",
    "    \"\"\"\n",
    "    def tokenizer_phrase(phrase: str) -> list[str]:\n",
    "    \n",
    "        phrase = phrase.lower()\n",
    "        phrase = re.sub(r\"[^\\w\\s]\", \"\", phrase)\n",
    "        tokens = phrase.split()\n",
    "        return tokens\n",
    "\n",
    "    #diviser les données en phrases\n",
    "    phrases = donnees.split('\\n')\n",
    "    phrases = [phrase.strip() for phrase in phrases if phrase.strip()]\n",
    "    \n",
    "    #diviser chaque phrase en tokens\n",
    "    phrases_tokenisees = [tokenizer_phrase(p) for p in phrases]\n",
    "    \n",
    "    return phrases, phrases_tokenisees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f0ea33-dd30-41bd-8d38-d3661f7096c7",
   "metadata": {},
   "source": [
    "|<h2>Extraction du vocabulaire à partir de données tokenisées</h2> |\n",
    "| :------------------: |\n",
    "| La fonction get_vocabulary prend en entrée une liste de listes de tokens (représentant des phrases tokenisées) et retourne un ensemble contenant tous les mots uniques présents dans ces données. Elle parcourt chaque sous-liste (correspondant à une phrase tokenisée) et ajoute chaque token unique à l'ensemble vocabulaire. Ainsi, à la fin, l'ensemble vocabulaire contient tous les mots uniques utilisés dans les données tokenisées.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f25fc15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(donnees_tokenisees: list[list[str]]) -> set[str]:\n",
    "    \"\"\"\n",
    "    Extrait le vocabulaire à partir d'une liste de données tokenisées.\n",
    "\n",
    "    Paramètres :\n",
    "        donnees_tokenisees (list[list[str]]) : Une liste de phrases tokenisées.\n",
    "\n",
    "    Retourne :\n",
    "        set[str] : Un ensemble contenant les mots uniques présents dans les données tokenisées.\n",
    "    \"\"\"\n",
    "    vocabulaire = set()\n",
    "    for sous_liste in donnees_tokenisees:\n",
    "        for element in sous_liste:\n",
    "            vocabulaire.add(element)\n",
    "    return vocabulaire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a7de1-035e-4ae3-94d5-017f46006e0c",
   "metadata": {},
   "source": [
    "|<h2>Division des données tokenisées en ensembles d'entraînement et de test</h2> |\n",
    "| :------------------: |\n",
    "| La fonction splitting_data prend en entrée une liste de listes de tokens donnees_tokenisees et divise ces données en deux ensembles : un ensemble d'entraînement et un ensemble de test. Le ratio entre ces deux ensembles est déterminé par le paramètre ratio_entrainement. Par défaut, 80% des données sont allouées à l'ensemble d'entraînement et 20% à l'ensemble de test.\n",
    "\n",
    "La fonction retourne un tuple contenant les deux ensembles, et elle offre également une option d'affichage pour montrer quelques exemples de chaque ensemble si le paramètre affichage est défini sur True..|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0586f3cb-fdec-443e-b3cf-375fe17e01e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_data(donnees_tokenisees: list[list[str]], ratio_entrainement: float=0.8,\n",
    "                affichage: bool=False) -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\"\n",
    "    Divise les données en ensembles d'entraînement et de test en fonction du ratio spécifié.\n",
    "    \n",
    "    Paramètres :\n",
    "        donnees_tokenisees (list[list[str]]) : Les données d'entrée sous forme d'une liste de listes de tokens.\n",
    "        ratio_entrainement (float) : Le ratio des données à utiliser pour l'entraînement (entre 0 et 1).\n",
    "        affichage (bool) : Si vrai, des informations sur la division seront affichées.\n",
    "        \n",
    "    Retourne :\n",
    "        Tuple[List[list[str]], List[list[str]]] : Les ensembles d'entraînement et de test sous forme de listes de phrases ou de séquences.\n",
    "    \"\"\"\n",
    "    taille_entrainement = int(len(donnees_tokenisees) * ratio_entrainement)\n",
    "    donnees_entrainement = donnees_tokenisees[:taille_entrainement]\n",
    "    donnees_test = donnees_tokenisees[taille_entrainement:]\n",
    "\n",
    "    if affichage:\n",
    "        print(f\"{len(donnees_tokenisees)} données sont divisées en {len(donnees_entrainement)} ensemble d'entraînement et {len(donnees_test)} ensemble de test\")\n",
    "\n",
    "        print(\"Premier échantillon d'entraînement :\")\n",
    "        print(donnees_entrainement[0])\n",
    "            \n",
    "        print(\"Premier échantillon de test :\")\n",
    "        print(donnees_test[0])\n",
    "    \n",
    "    return donnees_entrainement, donnees_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267af8d0-cb31-4f59-87b9-50759dd677af",
   "metadata": {},
   "source": [
    "|<h2>Vérification de la présence d'un mot dans le vocabulaire</h2> |\n",
    "| :------------------: |\n",
    "| La fonction isKnown prend en entrée un mot  et un ensemble de mots vocabulaire. Elle vérifie si le mot (après avoir été converti en minuscules pour éviter les différences de casse) est présent dans le vocabulaire. La fonction retourne True si le mot est présent dans le vocabulaire et False sinon.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "809e8451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isKnown(mot: str, vocabulaire: set[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Vérifie si un mot est présent dans le vocabulaire.\n",
    "\n",
    "    Paramètres :\n",
    "        mot (str) : Le mot à vérifier.\n",
    "        vocabulaire (set[str]) : L'ensemble de mots représentant le vocabulaire.\n",
    "\n",
    "    Retourne :\n",
    "        bool : True si le mot est dans le vocabulaire, False sinon.\n",
    "    \"\"\"\n",
    "    return mot.lower() in vocabulaire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e5aa6-db86-4559-8c5b-51d9e69c0a79",
   "metadata": {},
   "source": [
    "|<h2>Calcul de la distance d'édition minimale entre deux mots.</h2> |\n",
    "| :------------------: |\n",
    "|La fonction calculer_distance_edition détermine la distance d'édition minimale nécessaire pour transformer un mot source en un mot cible. La distance d'édition représente le nombre minimum d'opérations d'insertion, de suppression ou de remplacement nécessaires pour transformer le mot source en le mot cible.Les paramètres cout_insertion, cout_suppression, et cout_remplacement définissent le coût associé à chaque type d'opération. Par défaut, le coût d'insertion est de 1, le coût de suppression est de 1, et le coût de remplacement est de 2. La fonction retourne une matrice D qui stocke les distances d'édition intermédiaires pour chaque sous-séquence des mots source et cible, ainsi que la distance d'édition minimale med entre les deux mots.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d18f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculer_distance_edition(source: str, cible: str, cout_insertion: int=1, \n",
    "                            cout_suppression: int=1, cout_remplacement: int=2) -> tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Calcule la distance d'édition minimale entre deux mots.\n",
    "    \n",
    "    Paramètres :\n",
    "        source (str) : Le mot source.\n",
    "        cible (str) : Le mot cible.\n",
    "        cout_insertion (int) : Le coût de l'insertion (par défaut 1).\n",
    "        cout_suppression (int) : Le coût de la suppression (par défaut 1).\n",
    "        cout_remplacement (i nt) : Le coût du remplacement (par défaut 2).\n",
    "\n",
    "    Retourne :\n",
    "        D : une matrice de dimensions len(source)+1 par len(cible)+1 contenant les distances d'édition minimales\n",
    "        med : la distance d'édition minimale (med) nécessaire pour convertir la chaîne source en la cible\n",
    "    \"\"\"\n",
    "    m, n = len(source), len(cible)\n",
    "    D = np.zeros((m+1, n+1), dtype=int)\n",
    "    \n",
    "    for ligne in range(1, m+1):\n",
    "        D[ligne, 0] = D[ligne-1, 0] + cout_suppression\n",
    "        \n",
    "    for colonne in range(1, n+1):\n",
    "        D[0, colonne] = D[0, colonne-1] + cout_insertion\n",
    "        \n",
    "    for ligne in range(1, m+1):\n",
    "        for colonne in range(1, n+1):\n",
    "            cout_rempl = cout_remplacement\n",
    "            if source[ligne-1] == cible[colonne-1]:\n",
    "                cout_rempl = 0\n",
    "            D[ligne, colonne] = min(D[ligne-1, colonne] + cout_suppression, D[ligne, colonne-1] + cout_insertion, D[ligne-1, colonne-1] + cout_rempl)\n",
    "          \n",
    "    med = D[m, n]\n",
    "    \n",
    "    return D, med\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fef12a-f256-456e-9f52-310cf444d1e6",
   "metadata": {},
   "source": [
    "|<h2>Corrections possibles avec une seule modification.</h2> |\n",
    "| :------------------: |\n",
    "| La fonction edits1 génère un ensemble de corrections possibles pour un mot mal orthographié en considérant une seule modification (suppression, insertion, remplacement d'une lettre). Pour chaque type de modification, la fonction parcourt toutes les positions possibles où une modification peut être appliquée, puis génère toutes les corrections potentielles en utilisant l'alphabet. Ensuite, elle filtre ces corrections potentielles pour ne conserver que celles qui sont présentes dans le vocabulaire donné.La fonction retourne un ensemble contenant toutes les corrections possibles pour le mot mal orthographié.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b5cbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(mot: str, vocabulaire: list[str]) -> set[str]:\n",
    "    \"\"\"\n",
    "    Génère un ensemble de corrections possibles pour un mot mal orthographié avec une seule modification.\n",
    "\n",
    "    Paramètres :\n",
    "        mot (str) : Le mot mal orthographié.\n",
    "        vocabulaire (list[str]) : Une liste de mots du vocabulaire.\n",
    "\n",
    "    Retourne :\n",
    "        set[str] : Un ensemble de corrections possibles.\n",
    "    \"\"\"\n",
    "\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    decoupes = [(mot[:i], mot[i:]) for i in range(len(mot) + 1)]\n",
    "\n",
    "    suppressions = [gauche + droite[1:] for gauche, droite in decoupes if droite]\n",
    "    suppressions = [s for s in suppressions if est_mot(s, vocabulaire)]\n",
    "\n",
    "    insertions = [gauche + c + droite for gauche, droite in decoupes for c in alphabet]\n",
    "    insertions = [i for i in insertions if est_mot(i, vocabulaire)]\n",
    "\n",
    "    remplacements = [gauche + c + droite[1:] for gauche, droite in decoupes if droite for c in alphabet]\n",
    "    remplacements = [r for r in remplacements if est_mot(r, vocabulaire)]\n",
    "    \n",
    "    return set(suppressions + insertions + remplacements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aaa590-962b-46ea-9957-98e3d5cb5234",
   "metadata": {},
   "source": [
    "|<h2>Corrections possibles avec modification >= 1.</h2> |\n",
    "| :------------: |\n",
    "| La fonction edits2 génère un ensemble de corrections possibles pour un mot mal orthographié en considérant une ou plusieurs modification (suppression, insertion, remplacement d'une lettre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e0f9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits2(mot: str, vocabulaire: list[str], n_modifications: int=1, \n",
    "                        distance_max: int=2) -> set[str]:\n",
    "    \"\"\"\n",
    "    Génère une liste de corrections possibles pour un mot mal orthographié en fonction du vocabulaire donné et de la distance d'édition maximale.\n",
    "\n",
    "    Paramètres :\n",
    "        mot (str) : Le mot mal orthographié.\n",
    "        vocabulaire (list[str]) : Une liste de mots du vocabulaire.\n",
    "        n_modifications (int) : Le nombre de modifications autorisées pour générer des corrections (par défaut 1).\n",
    "        distance_max (int) : La distance d'édition maximale autorisée pour qu'une correction soit considérée (par défaut 2).\n",
    "\n",
    "    Retourne :\n",
    "        corrections_possibles (set[str]) : Un ensemble de corrections possibles.\n",
    "    \"\"\"\n",
    "    corrections_possibles = set()\n",
    "\n",
    "    if n_modifications == 1:\n",
    "        corrections_possibles = {corr for corr in edits1(mot, vocabulaire) if calculer_distance_edition(mot, corr)[1] <= distance_max}\n",
    "    else:\n",
    "        modifications_precedentes = {mot}\n",
    "        for _ in range(n_modifications):\n",
    "            modifications_actuelles = set()\n",
    "            for modification_precedente in modifications_precedentes:\n",
    "                nouvelles_corrections = {corr for corr in edits1(modification_precedente, vocabulaire) if calculer_distance_edition(mot, corr)[1] <= distance_max}\n",
    "                modifications_actuelles.update(nouvelles_corrections)\n",
    "            modifications_precedentes = modifications_actuelles\n",
    "        corrections_possibles = modifications_precedentes\n",
    "\n",
    "    return corrections_possibles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d24e0-bf17-4c3d-982c-7da4bacc7eba",
   "metadata": {},
   "source": [
    "|<h2>Comptage de nombre d'occurrences d'un mot dans les données tokenisées</h2> |\n",
    "| :------------------: |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98d52378-8e86-4e7b-8956-19116f7f82f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compter_mots(phrases_tokenisees: list[list[str]]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Compte le nombre d'apparitions de chaque mot dans les phrases tokenisées.\n",
    "\n",
    "    Paramètres :\n",
    "        sentences_tokenises (list[list[str]]) : Liste de listes de chaînes de caractères.\n",
    "\n",
    "    Retourne :\n",
    "        word_counts (dict[str, int]) : Dictionnaire qui fait correspondre chaque mot (str) à sa fréquence (int).\n",
    "    \"\"\"\n",
    "\n",
    "    word_counts = {}\n",
    "    for phrase in phrases_tokenisees:\n",
    "        for token in phrase:\n",
    "            if token not in word_counts.keys():\n",
    "                word_counts[token] = 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "\n",
    "    return word_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78292ddb-8f98-4a62-abd3-45a58c423111",
   "metadata": {},
   "source": [
    "|<h2>Traitement des mots hors vocabulaire (OOV)</h2> |\n",
    "| :------------------: |\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9164d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenir_mots_avec_frequence_superieure_ou_egale(phrases_tokenisees: list[list[str]], \n",
    "                                   seuil_frequence: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Trouve les mots qui apparaissent N fois ou plus.\n",
    "\n",
    "    Paramètres :\n",
    "        tokenized_sentences (list[list[str]]) : Liste de listes de phrases.\n",
    "        seuil_frequence (int) : Nombre minimum d'occurrences pour qu'un mot fasse partie du vocabulaire restreint.\n",
    "\n",
    "    Retourne :\n",
    "        closed_vocab (list[str]) : Liste des mots qui apparaissent N fois ou plus.\n",
    "    \"\"\"\n",
    "    closed_vocab = []\n",
    "    word_counts = compter_mots(phrases_tokenisees)\n",
    "    for word, cnt in word_counts.items():\n",
    "        if cnt >= seuil_frequence:\n",
    "            closed_vocab.append(word)\n",
    "\n",
    "    return closed_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "879f974e-b720-4640-9f2c-07e9e1969486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remplacer_mots_inconnus_par_unk(phrases_tokenisees: list[list[str]], \n",
    "                             vocabulaire: list[str], unknown_token: str=\"<unk>\"\n",
    "                             ) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Replace words not in the given vocabulary with the unknown token.\n",
    "\n",
    "    Parameters:\n",
    "        phrases_tokenisees: List of lists of strings\n",
    "        vocabulaire: List of strings that we will use\n",
    "        unknown_token: A string representing unknown (out-of-vocabulary) words\n",
    "    \n",
    "    Returns:\n",
    "        replaced_phrases_tokenisees: List of lists of strings, with words not in the vocabulary replaced\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulaire = set(vocabulaire)\n",
    "    replaced_phrases_tokenisees = []\n",
    "    for phrase in phrases_tokenisees:\n",
    "        replaced_phrase = []\n",
    "        for token in phrase:\n",
    "            if token in vocabulaire:\n",
    "                replaced_phrase.append(token)\n",
    "            else:\n",
    "                replaced_phrase.append(unknown_token)\n",
    "        replaced_phrases_tokenisees.append(replaced_phrase)\n",
    "        \n",
    "    return replaced_phrases_tokenisees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f0dabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_unk(phrases_tokenisees: list[list[str]], seuil_frequence: int):\n",
    "    \"\"\"\n",
    "    Prétraite les données en remplaçant les mots peu fréquents par le jeton inconnu.\n",
    "    \n",
    "    Paramètres :\n",
    "        données (list[list[str]]) : Liste de listes de chaînes de caractères.\n",
    "        freq_threshold (int) : Les mots dont le compte est inférieur à cette valeur sont considérés comme inconnus.\n",
    "\n",
    "    Retourne :\n",
    "        Tuple de\n",
    "        - données avec les mots peu fréquents remplacés par \"<unk>\"\n",
    "        - vocabulaire des mots qui apparaissent au moins n fois dans les données d'entraînement\n",
    "    \"\"\"\n",
    "   \n",
    "    vocabulaire = obtenir_mots_avec_frequence_superieure_ou_egale(phrases_tokenisees, seuil_frequence)\n",
    "    données_remplacées = remplacer_mots_inconnus_par_unk(phrases_tokenisees, vocabulaire)\n",
    "    \n",
    "    return données_remplacées, vocabulaire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f778641-359c-4a58-a621-034154023832",
   "metadata": {},
   "source": [
    "|<h2>Un model de langue n-gramme</h2> |\n",
    "| :------------------: |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b41cac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compter_n_grammes(donnees: list[list[str]], n: int, \n",
    "                      jeton_debut: str='<s>', jeton_fin: str= '</s>') -> dict:\n",
    "    \"\"\"\n",
    "    Compte tous les n-grammes dans les données fournies.\n",
    "    \n",
    "    Paramètres :\n",
    "        donnees (list[list[str]]) : Liste de listes de mots.\n",
    "        n (int) : nombre de mots dans une séquence (par défaut 2).\n",
    "        jeton_debut (str) : une chaîne de caractères indiquant le début de la phrase (par défaut '<s>').\n",
    "        jeton_fin (str) : une chaîne de caractères indiquant le fin de la phrase (par défaut '</s>').\n",
    "    \n",
    "    Retourne :\n",
    "        n_grammes (dict) : Un dictionnaire qui mappe un tuple de n mots à sa fréquence.\n",
    "    \"\"\"\n",
    "    n_grammes = {}\n",
    "\n",
    "    # Parcours des phrases dans les données\n",
    "    for phrase in donnees:\n",
    "        # Ajout des jetons de début et de fin à la phrase\n",
    "        phrase = [jeton_debut] * (n - 1) + phrase + [jeton_fin]\n",
    "        \n",
    "        # Génération des n-grammes pour la phrase\n",
    "        for i in range(len(phrase) - n + 1): \n",
    "            n_gramme = tuple(phrase[i:i+n])\n",
    "            \n",
    "            # Incrémentation du compteur du n-gramme\n",
    "            if n_gramme in n_grammes:\n",
    "                n_grammes[n_gramme] += 1\n",
    "            else:\n",
    "                n_grammes[n_gramme] = 1\n",
    "    \n",
    "    return n_grammes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc43f7d-cafc-4f88-b16a-1e690e259358",
   "metadata": {},
   "source": [
    "|<h2>Un modèle de langue avec Smoothing </h2> |\n",
    "| :------------------: |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "374c3553-1882-48d0-8b5e-f7b0e4135222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour estimer la probabilité d'un mot donné un contexte\n",
    "def language_model(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=0.0001):\n",
    "    \"\"\"\n",
    "    Estime la probabilité d'un mot donné un contexte.\n",
    "\n",
    "    Args:\n",
    "        word: str - Mot à prédire\n",
    "        n_gram: tuple - Contexte de mots précédents\n",
    "        n_gram_counts: dict - Comptes des n-grammes\n",
    "        n_plus1_gram_counts: dict - Comptes des (n+1)-grammes\n",
    "        vocabulary_size: int - Taille du vocabulaire\n",
    "        k: float - Constante de lissage (par défaut 0.01)\n",
    "\n",
    "    Returns:\n",
    "        float - Probabilité du mot donné le contexte\n",
    "    \"\"\"\n",
    "    n_gram = tuple(n_gram)\n",
    "    n_plus1_gram = n_gram + (word,)\n",
    "    \n",
    "    # Nombre d'occurrences du n-gramme et du (n+1)-gramme\n",
    "    count_n_gram = n_gram_counts[n_gram] if n_gram in n_gram_counts else 0\n",
    "    count_n_plus1_gram = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts else 0\n",
    "    \n",
    "    # Calcul de la probabilité lissée\n",
    "    numerator = count_n_plus1_gram + k\n",
    "    denominator = count_n_gram + k * vocabulary_size\n",
    "    \n",
    "    probability = numerator / denominator\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462f207-52e4-48aa-93ef-4f4e0feba337",
   "metadata": {},
   "source": [
    "|<h2>Correction d'un texte avec un modèle bigramme</h2> |\n",
    "| :------------------: |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f67b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(texte: str, vocabulaire: set[str], top_n: int=4, n_g: int=2,\n",
    "               n_edits: int=1, max_distance: int=2) -> tuple[dict, str]:\n",
    "    \"\"\"\n",
    "    Effectue une correction orthographique sur le texte donné en utilisant un modèle de langue n-gramme.\n",
    "\n",
    "    Paramètres :\n",
    "        texte (str) : Le texte sur lequel effectuer la correction orthographique.\n",
    "        vocabulaire (set[str]) : Un ensemble de mots représentant le vocabulaire.\n",
    "        top_n (int) : Le nombre de suggestions les plus probables à prendre en compte (par défaut 2).\n",
    "        n_g (int) : L'ordre du modèle de langue n-gramme (par défaut 2).\n",
    "        k (int) : Constante positive, paramètre de lissage (par défaut 1.0).\n",
    "        n_edits (int) : Le nombre maximum de modifications autorisées dans une correction suggérée (par défaut 1).\n",
    "        max_distance (int) : La distance maximale de modification autorisée pour une correction suggérée (par défaut 2).\n",
    "\n",
    "    Retourne :\n",
    "        sorted_dict (dict) : Dictionnaire de suggestions.\n",
    "        texte_corrige (str) : La version corrigée du texte d'entrée.\n",
    "    \"\"\"\n",
    "   \n",
    "    n_meilleures = []\n",
    "    suggestions = dict()\n",
    "    # Traitement du texte\n",
    "    phrases, phrases_tokenisees = process_data(texte)\n",
    "    # Comptage des n-grammes\n",
    "    n_grammes = compter_n_grammes(phrases_tokenisees, n_g)\n",
    "    n_plus1_grammes = compter_n_grammes(phrases_tokenisees, n_g+1)\n",
    "    texte_corrige = texte\n",
    "    \n",
    "    # Parcours des phrases tokenisées\n",
    "    for phrase in phrases_tokenisees:\n",
    "        # Création d'une copie de la phrase avec des marqueurs \n",
    "        # de début et de fin pour faciliter la gestion des n-grammes.\n",
    "        index = None\n",
    "        phrase_tmp = ['<s>']*(n_g-1) + phrase + ['</s>']\n",
    "        \n",
    "        \n",
    "        for token in phrase:\n",
    "            probas = dict()\n",
    "            # Vérification si le token est connu\n",
    "            if not isKnown(token, vocabulaire):\n",
    "                index = phrase_tmp.index(token)\n",
    "                n_gramme_precedent = tuple(phrase_tmp[abs(index-n_g):index])  \n",
    "                comptes_n_grammes = n_grammes.get(n_gramme_precedent, {}) \n",
    "                corrections = edits2(token, vocabulaire, n_edits, max_distance)\n",
    "                corrections = [c for c in corrections if est_mot(c, vocabulaire)]\n",
    "                # Calcul des probabilités des corrections\n",
    "                for corr in corrections:\n",
    "                    proba = language_model(corr, n_gramme_precedent, n_grammes,\n",
    "                                           n_plus1_grammes, len(vocabulaire))\n",
    "                    probas[corr] = proba\n",
    "                suggestions[token] = probas\n",
    "        \n",
    "        # Tri des suggestions par probabilité\n",
    "        suggestions_triees = {k: dict(sorted(v.items(), key=lambda item: item[1], reverse=True)) for k, v in suggestions.items()}\n",
    "        sorted_dict = {}\n",
    "        # Sélection des meilleures t suggestions\n",
    "        for cle, dict_interne in suggestions_triees.items():\n",
    "            dict_interne_trie = dict(sorted(dict_interne.items(), key=lambda item: item[1], reverse=True)[:top_n])\n",
    "            sorted_dict[cle] = dict_interne_trie\n",
    "\n",
    "        # Remplacement des mots dans le texte corrigé\n",
    "        for cle in sorted_dict.keys():\n",
    "            if cle in texte_corrige:\n",
    "                if sorted_dict[cle]:  # Vérifier si le dictionnaire n'est pas vide\n",
    "                    premier_mot_interne = next(iter(sorted_dict[cle]))\n",
    "                    texte_corrige = texte_corrige.replace(cle, premier_mot_interne)\n",
    "\n",
    "    # Retourner le dictionnaire de suggestions et le texte corrigé\n",
    "    return sorted_dict, texte_corrige\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9fb3a5-6f0a-472f-8bea-62268ccb5874",
   "metadata": {},
   "source": [
    "|<h2>Application</h2> |\n",
    "| :------------------: |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41977921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le texte original:\n",
      "Hallo there, I wqs tryinj to finich this wirk !\n",
      "\n",
      "Le texte corrigé:\n",
      "Hallo there, I ws trying to finish this work !\n",
      "\n",
      "Les mots mal orthographiés et leurs corrections:\n",
      "----------------------------------------------\n",
      "wqs:\n",
      "ws:\t2.0203652820429932e-05\n",
      "was:\t2.0203652820429932e-05\n",
      "----------------------------------------------\n",
      "tryinj:\n",
      "trying:\t2.0203652820429932e-05\n",
      "----------------------------------------------\n",
      "finich:\n",
      "finish:\t2.0203652820429932e-05\n",
      "----------------------------------------------\n",
      "wirk:\n",
      "work:\t2.0203652820429932e-05\n",
      "kirk:\t2.0203652820429932e-05\n",
      "dirk:\t2.0203652820429932e-05\n",
      "wick:\t2.0203652820429932e-05\n"
     ]
    }
   ],
   "source": [
    "donnees = load_data('C:\\\\Users\\\\hp\\\\Downloads\\\\NLP_DL\\\\big.txt')\n",
    "phrases, donnees_tokenisees = process_data(donnees)\n",
    "vocabulaire = get_vocabulary(donnees_tokenisees)\n",
    "\n",
    "n_gr = 2\n",
    "n_grams=compter_n_grammes(training_data, n_gr)\n",
    "n1_grams=compter_n_grammes(training_data, n_gr+1)\n",
    "vocabulary_size = len(n_grams)\n",
    "\n",
    "text = \"Hallo there, I wqs tryinj to finich this wirk !\"\n",
    "\n",
    "sorted_dict, texte_corrige = correction(text, vocabulaire)\n",
    "\n",
    "print(f\"Le texte original:\\n{text}\\n\")\n",
    "print(f\"Le texte corrigé:\\n{texte_corrige}\\n\")\n",
    "print(f\"Les mots mal orthographiés et leurs corrections:\")\n",
    "for mot in sorted_dict.keys():\n",
    "    print('-'*46)\n",
    "    print(f\"{mot}:\")\n",
    "    for c, p in sorted_dict[mot].items():\n",
    "        print(f\"{c}:\\t{p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f0ce2-c056-4d9b-9b6b-d1d58d444890",
   "metadata": {},
   "source": [
    "|<h2>Evaluation du modèle de langue par la mesure de perplexité</h2> |\n",
    "| :------------------: |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ff7d1e2-f64b-4f3e-9fc9-e62e2985fab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexité pour le texte de test: 7.0966650225862224\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=0.0001):\n",
    "    \"\"\"\n",
    "    Calcule la perplexité pour une liste de phrases.\n",
    "\n",
    "    Args:\n",
    "        sentence: List[str] - Liste de mots dans la phrase\n",
    "        n_gram_counts: dict - Comptes des n-grammes\n",
    "        n_plus1_gram_counts: dict - Comptes des (n+1)-grammes\n",
    "        vocabulary_size: int - Taille du vocabulaire\n",
    "        k: float - Constante de lissage (par défaut 0.001)\n",
    "\n",
    "    Returns:\n",
    "        float - Perplexité\n",
    "    \"\"\"\n",
    "    n = len(list(n_gram_counts.keys())[0])  # Longueur des n-grammes\n",
    "    sentence = [\"<s>\"] * (n-1) + sentence + [\"</s>\"]  # Ajout de marqueurs de début et de fin\n",
    "    sentence = tuple(sentence)\n",
    "    N = len(sentence)\n",
    "    product_pi = 1.0\n",
    "    \n",
    "    for t in range(n, N):  # Parcours de chaque mot dans la phrase\n",
    "        n_gram = sentence[t-n:t]\n",
    "        word = sentence[t]\n",
    "        probability = language_model(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n",
    "        product_pi *= 1 / probability\n",
    "\n",
    "    perplexity = product_pi**(1/float(N))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "donnees = load_data('C:\\\\Users\\\\hp\\\\Downloads\\\\NLP_DL\\\\big.txt')\n",
    "phrases, donnees_tokenisees = process_data(donnees)\n",
    "training_data, test_data = splitting_data(donnees_tokenisees)\n",
    "\n",
    "# Calcul des comptes des n-grammes et (n+1)-grammes\n",
    "n_gr = 2\n",
    "n_grams=compter_n_grammes(test_data, n_gr)\n",
    "n1_grams=compter_n_grammes(test_data, n_gr+1)\n",
    "vocabulary_size = len(n_grams)\n",
    "\n",
    "# Calcul de la perplexité pour le texte de test\n",
    "perplexity_test = 0\n",
    "for sentence in test_data:\n",
    "    perplexity_test = perplexity_test + calculate_perplexity(sentence, n_grams, n1_grams, vocabulary_size)\n",
    "print(\"Perplexité pour le texte de test:\", perplexity_test/len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8035d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f1d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b31c9ca-a81c-4573-aa7f-16ba45cfa867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423f5d6-4621-4712-9523-eb997ee95663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59893f-31cb-46bf-b02b-1d3a7be63c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d3f13-7b04-4cd6-8569-1b8bf363bf4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f9df8-20c9-4ba7-ae58-deab0f6c78b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f14ade-450a-4fa3-8181-1b28321ff472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30776ded-0106-488e-a581-a5749517e8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2ba1db-d6ed-46d7-8d4e-384c70c1cb29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f6d33d-017f-452a-9b2b-35983ca46994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
