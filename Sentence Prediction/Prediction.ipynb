{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dffcedee-5cd4-4c38-ad8c-70dfcd08eeae",
   "metadata": {},
   "source": [
    "# AutoCprrector Version 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84704ee8-61be-4652-a8d9-ee96f35cd9e0",
   "metadata": {},
   "source": [
    "|<h1>Preparation des données</h1> |\n",
    "| :------------------: |\n",
    "| process_data : Divise le corpus en phrases et les phrases en mots.\n",
    "count_words : Compte la fréquence de chaque mot.\n",
    "get_words_with_frequency_above_or_equal N: Identifie les mots fréquents.\n",
    "prepare_data : Remplace les mots rares par un jeton spécifié et retourne des nouvelles phrases tokenisées et traitées|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ee9cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "\n",
    "def process_data(data: str) -> Tuple[List[str], List[List[str]]]:\n",
    "    \"\"\"\n",
    "    Tokenize les données en phrases et en mots.\n",
    "\n",
    "    Paramètres :\n",
    "        data (str) : Les données d'entrée sous forme de chaîne de caractères.\n",
    "\n",
    "    Retourne :\n",
    "        Tuple[List[str], List[List[str]]] : Un tuple contenant la liste des phrases et la liste des phrases tokenisées.\n",
    "    \"\"\"\n",
    "    def tokenizer_phrase(phrase: str) -> List[str]:\n",
    "        phrase = phrase.lower()\n",
    "        phrase = re.sub(r\"[^\\w\\s]\", \"\", phrase)\n",
    "        tokens = phrase.split()\n",
    "        return tokens\n",
    "\n",
    "    phrases = data.split('\\n')\n",
    "    phrases = [phrase.strip() for phrase in phrases if phrase.strip()]\n",
    "    phrases_tokenisees = [tokenizer_phrase(p) for p in phrases]\n",
    "    \n",
    "    return phrases, phrases_tokenisees\n",
    "\n",
    "def count_words(phrases_tokenisees: List[List[str]]) -> dict:\n",
    "    \"\"\"\n",
    "    Compte le nombre d'apparitions de chaque mot dans les phrases tokenisées.\n",
    "\n",
    "    Paramètres :\n",
    "        sentences_tokenises (list[list[str]]) : Liste de listes de chaînes de caractères.\n",
    "\n",
    "    Retourne :\n",
    "        word_counts (dict[str, int]) : Dictionnaire qui fait correspondre chaque mot (str) à sa fréquence (int).\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    for phrase in phrases_tokenisees:\n",
    "        for token in phrase:\n",
    "            if token not in word_counts.keys():\n",
    "                word_counts[token] = 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "def get_words_with_frequency_above_or_equal(phrases_tokenisees: List[List[str]], \n",
    "                               threshold_frequency: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Trouve les mots qui apparaissent N fois ou plus.\n",
    "\n",
    "    Paramètres :\n",
    "        tokenized_sentences (list[list[str]]) : Liste de listes de phrases.\n",
    "        threshold_frequency (int) : Nombre minimum d'occurrences pour qu'un mot fasse partie du vocabulaire restreint.\n",
    "\n",
    "    Retourne :\n",
    "        closed_vocab (list[str]) : Liste des mots qui apparaissent N fois ou plus.\n",
    "    \"\"\"\n",
    "    closed_vocab = []\n",
    "    word_counts = count_words(phrases_tokenisees)\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= threshold_frequency:\n",
    "            closed_vocab.append(word)\n",
    "\n",
    "    return closed_vocab\n",
    "\n",
    "def prepare_data(phrases_tokenisees: List[List[str]], \n",
    "                         closed_vocab: List[str], unknown_token: str=\"<unk>\"\n",
    "                         ) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Replace les mots qui ne sont pas dans le vocabulaire donné par le jeton inconnu.\n",
    "\n",
    "    Paramètres:\n",
    "        phrases_tokenisees: Liste de listes de chaînes de caractères\n",
    "        closed_vocab: Liste de chaînes de caractères que nous utiliserons\n",
    "        unknown_token: Une chaîne de caractères représentant les mots inconnus (hors vocabulaire)\n",
    "    \n",
    "    Retourne:\n",
    "        replaced_phrases_tokenisees: Liste de listes de chaînes de caractères, avec les mots qui ne sont pas dans le vocabulaire remplacés\n",
    "    \"\"\"\n",
    "    closed_vocab = set(closed_vocab)  # Convertir en ensemble\n",
    "    replaced_phrases_tokenisees = []\n",
    "    for phrase in phrases_tokenisees:\n",
    "        replaced_phrase = []\n",
    "        for token in phrase:\n",
    "            if token in closed_vocab:\n",
    "                replaced_phrase.append(token)\n",
    "            else:\n",
    "                replaced_phrase.append(unknown_token)\n",
    "        replaced_phrases_tokenisees.append(tuple(replaced_phrase))  # Convertir en tuple\n",
    "        \n",
    "    return replaced_phrases_tokenisees\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db5b3f1-6603-4ef1-8bf2-86f454e54f84",
   "metadata": {},
   "source": [
    "|<h1>Entrainement des modèles n-grames </h1> |\n",
    "| :------------------: |\n",
    "|\n",
    "<h3>count_n_grams() : </h3> Cette fonction compte tous les n-grammes dans les données fournies. Elle prend une liste de tuples de mots, un entier n représentant la taille de l'n-gramme, ainsi que des tokens de début et de fin de phrase optionnels. Elle retourne un dictionnaire qui mappe chaque n-gramme à sa fréquence.\r",
    "<h3>train(): </h3> Cette fonction entraîne un modèle de n-gramme avec un lissage add-k à partir d'un fichier d'entrée. Elle lit d'abord le contenu du fichier, puis prétraite les données en tokenisant les phrases, en remplaçant les mots hors vocabulaire, et en comptant les n-grammes. Ensuite, elle calcule les probabilités de transition avec le lissage add-k et les stocke dans un dictionnaire, qui est ensuite retourn..|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57d6fe83-3808-4fc8-ad13-7548235dd2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data: List[Tuple[str]], n: int, start_token: str='<s>', end_token: str= '</s>') -> dict:\n",
    "    \"\"\"\n",
    "    Compte tous les n-grammes dans les données fournies.\n",
    "    \n",
    "    Paramètres :\n",
    "        data (list[tuple[str]]) : Liste de tuples de mots.\n",
    "        n (int) : nombre de mots dans une séquence (n-gramme).\n",
    "        start_token (str) : une chaîne de caractères indiquant le début de la phrase (par défaut '<s>').\n",
    "        end_token (str) : une chaîne de caractères indiquant le fin de la phrase (par défaut '</s>').\n",
    "    \n",
    "    Retourne :\n",
    "        n_grams (dict) : Un dictionnaire qui mappe un tuple de n mots à sa fréquence.\n",
    "    \"\"\"\n",
    "    n_grams = {}\n",
    "\n",
    "    for phrase in data:\n",
    "        phrase = (start_token,) * (n - 1) + phrase + (end_token,)\n",
    "        for i in range(len(phrase) - n + 1): \n",
    "            n_gram = phrase[i:i+n]\n",
    "            n_gram = tuple(phrase[i:i+n])\n",
    "            if n_gram in n_grams:\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "    \n",
    "    return n_grams\n",
    "\n",
    "def train(infile: str, ngram_size: int, k: float) -> dict:\n",
    "    \"\"\"\n",
    "    Entraîne un modèle de n-gramme avec lissage add-k à partir d'un fichier d'entrée.\n",
    "    \"\"\"\n",
    "    with open(infile, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = file.read()\n",
    "\n",
    "    _, phrases_tokenisees = process_data(data)\n",
    "    vocab = get_words_with_frequency_above_or_equal(phrases_tokenisees, 1)\n",
    "    replaced_phrases = prepare_data(phrases_tokenisees, vocab)\n",
    "\n",
    "    ngrams = count_n_grams(replaced_phrases, ngram_size)\n",
    "    vocab_size = len(set(word for phrase in phrases_tokenisees for word in phrase))\n",
    "\n",
    "    # Calcul des probabilités avec lissage add-k et normalisation\n",
    "    ngram_probs = {}\n",
    "    for ngram, count in ngrams.items():\n",
    "        context = \" \".join(ngram[:-1])\n",
    "        word = ngram[-1]\n",
    "        context_count = sum(value for key, value in ngrams.items() if key[:-1] == ngram[:-1])\n",
    "        prob = (count + k) / (context_count + k * vocab_size)\n",
    "        \n",
    "        if context in ngram_probs:\n",
    "            ngram_probs[context][word] = prob \n",
    "        else:\n",
    "            ngram_probs[context] = {word: prob }\n",
    "\n",
    "   \n",
    "\n",
    "    return ngram_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2965ab-45a3-4f64-97aa-75ca4d25ae80",
   "metadata": {},
   "source": [
    "|<h2>Prédiction d'une phrase </h2> |\n",
    "| :------------------: |\n",
    "|\n",
    "<h3>predict_ngram() : </h3> Cette fonction est utilisée pour calculer la probabilité d'une phrase donnée en utilisant un modèle de n-gramme. \n",
    "Elle commence par normaliser la phrase d'entrée en la divisant en tokens et en les convertissant en minuscules. Ensuite, elle itère à travers les n-grammes de la phrase (en prenant en compte la taille de l'n-gramme spécifiée). Pour chaque n-gramme, elle récupère le contexte et le mot actuel, et vérifie s'ils existent dans le modèle de n-gramme. Si c'est le cas, elle ajoute le logarithme de la probabilité du n-gramme au total des probabilités logarithmiques. Enfin, elle retourne la somme totale des probabilités logarithmiques calculées.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd7dd7e4-8ec4-447a-91ea-df52251e363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ngram(sentence: str, ngram_model: dict, ngram_size: int) -> float:\n",
    "    \"\"\"\n",
    "    Calcule la probabilité d'une phrase donnée en utilisant un modèle de n-gramme.\n",
    "\n",
    "    Paramètres :\n",
    "        sentence (str) : La phrase dont on veut calculer la probabilité.\n",
    "        ngram_model (dict) : Le modèle de n-gramme entraîné, contenant les probabilités des n-grammes.\n",
    "        ngram_size (int) : La taille des n-grammes à utiliser dans le modèle.\n",
    "\n",
    "    Retourne :\n",
    "        float : La probabilité logarithmique de la phrase selon le modèle de n-gramme.\n",
    "    \"\"\"\n",
    "    # Normalisation de la phrase\n",
    "    tokens = sentence.lower().split()\n",
    "    n = len(tokens)\n",
    "\n",
    "    total_log_prob = 0.0\n",
    "    for i in range(ngram_size - 1, n):\n",
    "        context = \" \".join(tokens[i - ngram_size + 1:i])\n",
    "        word = tokens[i]\n",
    "        if context in ngram_model and word in ngram_model[context]:\n",
    "            total_log_prob += math.log(ngram_model[context][word])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aae39c-0404-4d15-840e-0b780c6da78f",
   "metadata": {},
   "source": [
    "|<h2>Evaluer le modèle à travers la perplexité </h2> |\n",
    "| :------------------: |\n",
    "|\n",
    "<h3>normalize_log_prob() : </h3>Cette fonction prend une probabilité logarithmique en entrée et la normalise en la transformant en probabilité non logarithmique. Elle utilise la fonction d'exponentielle (math.exp) pour cela.\r",
    "<h3>\n",
    "calculate_perplexiy() : </h3> Cette fonction calcule la perplexité d'un modèle de langue n-gramme sur un fichier de test. Elle prend en entrée le chemin du fichier de test, le modèle de langue n-gramme entraîné, la taille des n-grammes utilisée dans le modèle, et le vocabulaire utilisé pour remplacer les mots hors vocabulaire. La fonction lit chaque ligne du fichier de test, prétraite la phrase en la normalisant et en remplaçant les mots hors vocabulaire, puis calcule la probabilité logarithmique de chaque phrase selon le modèle de n-gramme. Enfin, elle utilise ces probabilités pour calculer la perplexité globale du modèle sur le fichier de test, qui est ensuite retourné.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "763a4d0c-8f94-4703-ace3-6299b10cd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_log_prob(log_prob: float) -> float:\n",
    "    \"\"\"\n",
    "    Normalise la probabilité logarithmique en soustrayant le logarithme de la somme des logarithmes des probabilités.\n",
    "\n",
    "    Paramètres :\n",
    "        log_prob (float) : La probabilité logarithmique à normaliser.\n",
    "\n",
    "    Retourne :\n",
    "        float : La probabilité normalisée.\n",
    "    \"\"\"\n",
    "    return math.exp(log_prob)\n",
    "\n",
    "def calculate_perplexity(test_file: str, ngram_model: dict, ngram_size: int, vocab: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calcule la perplexité d'un modèle de langue n-gramme sur un fichier de test.\n",
    "\n",
    "    Paramètres :\n",
    "        test_file (str) : Le chemin du fichier de test.\n",
    "        ngram_model (dict) : Le modèle de langue n-gramme entraîné.\n",
    "        ngram_size (int) : La taille des n-grammes utilisée dans le modèle.\n",
    "        vocab (List[str]): Le vocabulaire utilisé pour remplacer les mots hors vocabulaire.\n",
    "\n",
    "    Retourne :\n",
    "        float : La perplexité calculée.\n",
    "    \"\"\"\n",
    "    total_log_prob = 0.0\n",
    "    total_words = 0\n",
    "\n",
    "    with open(test_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            # Prétraitement de la phrase\n",
    "            sentence = line.strip().lower()\n",
    "            tokens = sentence.split()\n",
    "            tokens = [token if token in vocab else \"<unk>\" for token in tokens]\n",
    "            \n",
    "            # Calcul de la probabilité logarithmique de la phrase\n",
    "            for i in range(ngram_size - 1, len(tokens)):\n",
    "                context = \" \".join(tokens[i - ngram_size + 1:i])\n",
    "                word = tokens[i]\n",
    "                if context in ngram_model and word in ngram_model[context]:\n",
    "                    total_log_prob += math.log(ngram_model[context][word])\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "                total_words += 1\n",
    "    \n",
    "    # Calcul de la perplexité\n",
    "    perplexity = math.exp(-total_log_prob / total_words)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e969d95c-2080-4622-87cb-bf7b8911bbb4",
   "metadata": {},
   "source": [
    "|<h1>Application</h1> |\n",
    "| :------------------: |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "beff7e2d-2d6e-45d3-a06e-8b25a04a7856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probability of the sentence 'NOT IN A BOX': -3.9223\n",
      "Probability of the sentence 'NOT IN A BOX': 0.0198\n",
      "Perplexity on test data: 1.8314\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    infile = \"C:\\\\Users\\\\hp\\\\Downloads\\\\TP2_NLP\\\\PA_files\\\\ngramv1.train\"\n",
    "    test_file = \"C:\\\\Users\\\\hp\\\\Downloads\\\\TP2_NLP\\\\PA_files\\\\ngramv1.test\"\n",
    "    test_sentence = \"NOT IN A BOX\"\n",
    "    ngram_size = 2\n",
    "    k = 0.01  # Valeur arbitraire pour le lissage add-k\n",
    "\n",
    "    # Entraîner le modèle de n-gramme\n",
    "    ngram_model = train(infile, ngram_size, k)\n",
    "\n",
    "    # Utiliser la fonction predict_ngram pour prédire la probabilité logarithmique de la phrase de test\n",
    "    log_sentence_prob = predict_ngram(test_sentence, ngram_model, ngram_size)\n",
    "    print(f\"Log Probability of the sentence '{test_sentence}': {log_sentence_prob:.4f}\")\n",
    "\n",
    "    # Normaliser la probabilité logarithmique\n",
    "    sentence_prob = normalize_log_prob(log_sentence_prob)\n",
    "    print(f\"Probability of the sentence '{test_sentence}': {sentence_prob:.4f}\")\n",
    "\n",
    "    # Calculer la perplexité sur les données de test\n",
    "    vocab = get_words_with_frequency_above_or_equal(process_data(open(infile).read())[1], 1)\n",
    "    perplexity = calculate_perplexity(test_file, ngram_model, ngram_size, vocab)\n",
    "    print(f\"Perplexity on test data: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b094e279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b41c3-9a71-40ca-9b48-3c67dcdec59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55c970-f1d1-4f97-9e40-0397f69a91e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8094fb-cb38-4a7c-9d58-d85303cc84bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
